In recent years, the increasingly popular video surveillance network has covered most public places in
cities. Because of the popular video surveillance network, peopleâ€™s lives and property safety have been
more fully guaranteed. But countless video data is generated every day. A large number of
surveillance videos have brought huge storage pressure. At the same time, when staff traverse a large
amount of surveillance video content, a lot of manpower will be wasted and the efficiency will be low.
In the massive surveillance video, not all scenes contain valid information at all times. In fact, areas
with low personnel mobility, such as server rooms, corridors in buildings with small traffic, etc., may
remain unchanged for most of the time under monitoring. These videos with no change time do not
contain useful information, but take up a lot of storage space and consume a lot of manpower when
traversing. If we can automatically remove these useless video clips and only keep the potentially
useful video clips and the key frame images in the clips, the pressure of storage and traversal can be
greatly reduced.
In this work, we use a deep learning method to extract video frame features using an auto-encoder
network. By calculating the difference between the features, we can automatically remove the useless
parts of the video, and only keep the video clips and key frames that contain useful information

2. Proposed Approach
There are many ways to extract key video clips and key frames from a video. In the field of deep
learning, we can use the method of object detection to detect the targets in the video, save the video
segment when the target appears, or extract the video feature through the auto-encoders, and retain the
video segment when the feature changes significantly. In the actual application scenario considered,
when an event occurs, there may not necessarily be a target in the frames, but it may also be other
emergencies such as facility damage, environmental abnormality, etc. Therefore, this paper uses the
auto-encoders to extract the features, and retains the video key segments and key frame images by
comparing the changes of the features. In this chapter, we will introduce the methods that will be used
in this article.
2.1. Loss Function for the Network
To train an auto-encoder network, we need a loss function to measure the results. In order to make the
features extracted by the auto-encoder network contain all the features of the input image as much as
possible, we hope that the output of the auto-encoder can be as similar as possible to the original input
image. In other words, we hope that the features extracted from the auto-encoder can be restored to the
original image. In evaluating the similarity of image restoration, commonly used loss functions
include L2 loss function, SSIM loss function, MS-SSIM loss function, and loss function combined
with MS-SSIM and L1.
L2 loss function
is a common loss function used to compare difference between two images,
and is often used to measure the error of image restoration [1-3], but its performance is poorly
correlated with the perception of human observers [4]. The sensitivity of the human visual system to
noise depends more on local luminance, contrast, and structure [5].
The SSIM loss function
is derived from the comparison standard SSIM[5] of image
similarity, and is often used for image compression [6], image reconstruction [7], image denoising and
super-resolution [8] tasks. The SSIM method compares the similarity of two images from three
aspects, namely the luminance, contrast and structure of the image.
The MS-SSIM standard [9] is a multi-scale version of the SSIM standard. MS-SSIM loss function
has a good performance in the restoration ability of the edge and local area of the image.
Although the loss functions of MS-SSIM and SSIM restore clearer images, the restoration of colors
is relatively poor. So we can combine MS-SSIM loss function with the L1 loss function as
to
get more accurate color reproduction [10].
2.2. Structure of the Network
After selecting the loss function of the auto-encoder network, we need to confirm the basic structure of
the auto-encoder network. In order to ensure the speed of extracting video features, we hope to reduce
the amount of network calculation and the size of features as much as possible. Therefore, we
compared the effects of the network in terms of the number of channels, input size, and feature size of
the network. First, we compare the effect of reducing the number of channels on the network. Finally,
we fixed the input size and reduced the feature size to compare the effects of different feature sizes on
the network effect.
2.3. Training Method for the Network
After confirming the basic structure of the self-encoding network, we compared the training methods
of the network in order to achieve better results on the current network structure. First, we compared
the effects of training for each camera individually and training a generalized network directly on a
large data set. Then, we confirmed whether the network should be pre-trained with a large data set.

